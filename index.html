<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yeming Wen</title>
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <style>
        body, td, th, p, a {
          font-family: 'Lato', sans-serif;
          font-size: 16px;
          color: #333;
        }

        /* Increase overall page width */
        body {
          max-width: 880px; /* Adjust this value to increase the page width */
          margin: auto; /* Centers the content */
        }

        /* If using a table or a specific container for layout, adjust its width as well */
        table {
          width: 100%; /* Allows the table to expand up to the body's max-width */
          max-width: 880px; /* You can adjust this to match the body's max-width or as needed */
        }

        /* Ensure content within the table cells also respects the new layout */
        td, th {
          padding: 10px; /* Adjust padding for better spacing */
        }

        a, a:visited {
          color: #1772d0;
          text-decoration: none;
        }

        a:hover, a:focus {
          color: #f09228;
          text-decoration: underline;
        }

        .container {
          max-width: 880px;
          margin: auto;
          padding: 20px;
        }

        header, footer {
          text-align: center;
          padding: 20px 0;
        }

        .profile {
          display: flex;
          align-items: center;
          justify-content: center;
          flex-wrap: wrap;
          gap: 25px;
        }

        .profile-info {
          flex: 1;
          min-width: 300px;
        }

        .profile-photo img {
          max-width: 160px;
          border-radius: 80px;
        }

        .publications {
          margin-top: 20px;
          margin-bottom: 50px;
        }

        .publication {
          margin-bottom: 30px;
        }

        .publication img {
          max-width: 100%;
          height: auto;
          margin-bottom: 10px;
        }

        .publication-entry {
          display: flex;
          align-items: stretch; /* Align items to stretch to fill the container */
          gap: 20px;
        }

        .publication-image {
          flex: 0 1 38%; /* Allows the image container to shrink but not grow, with a basis of 20% */
          display: flex;
          align-items: center; /* This will vertically center the image within its container */
        }

        .publication-image img {
          width: 100%; /* Ensures the image takes the full width of its parent */
          height: auto; /* Maintains the aspect ratio of the image */
          object-fit: contain; /* Ensures the image is fully visible, might leave some space if the aspect ratio doesn't match */
        }

        .publication-content {
          flex: 1; /* Allows the text content to take the remaining space */
        }

        .spotlight {
          color: red;
        }


        @media (max-width: 600px) {
          .profile {
            flex-direction: column;
          }

          .profile-photo {
            margin-bottom: 20px;
          }
        }
    </style>
</head>
<body>
<div class="container">
    <header>
        <h1>Yeming Wen</h1>
    </header>

    <section class="profile">
        <div class="profile-info">
            <p>I am a computer science PhD at UT Austin, advised by <a href="https://www.cs.utexas.edu/~swarat/">Prof. Swarat Chaudhuri</a>. My research focuses on building a machine learning framework to generate code with human-like efficiency. In the meantime, I'm also interested in enhancing the efficient adaptation framework for Large Language Models (LLMs), specifically for code generation tasks. Before joining UT Austin, I was a master student in computer science advised by <a href="https://jimmylba.github.io/"> Prof. Jimmy Ba</a> at University of Toronto. I worked on the development of efficient learning algorithms for deep neural networks. My undergraduate studies were also at the University of Toronto, working with <a href="https://www.cs.toronto.edu/~rgrosse/"> Prof. Roger Grosse</a> on stochastic deep neural networks.</p>
            <p><a href="mailto:ywen@utexas.edu">Email</a> &nbsp/&nbsp
               <a href="ywen_cv.pdf">CV</a> &nbsp/&nbsp
               <a href="https://scholar.google.ca/citations?user=J2GzNAkAAAAJ&hl=en">Scholar</a> &nbsp/&nbsp
               <a href="https://www.linkedin.com/in/yeming-wen-0480a6a2/">LinkedIn</a></p>
        </div>
        <div class="profile-photo">
            <img src="ywen.jpg" alt="Yeming Wen">
        </div>
    </section>

    <section class="publications">
        <h2>Preprints</h2>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2402.08073.pdf">Grounding Data Science Code Generation with Input-Output Specifications</a></h3>
                <p><strong>Yeming Wen</strong>, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri & Alex Polozov <br>
                <em>Instruction Tuning and Instruction Following Workshop at NeurIPS, 2023</em></p> 
                <p>We enhance large language models for code generation from natural language prompts with explicit input-output specifications by leveraging synthetic data and utilizing execution-derived feedback, markedly improving alignment with user specifications in data science tasks.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2310.04353.pdf">An In-Context Learning Agent for Formal Theorem-Proving</a></h3>
                <p>Amitayush Thakur, George Tsoukalas, <strong>Yeming Wen</strong>, Jimmy Xin & Swarat Chaudhuri<br>
                <em>Math-AI Workshop at NeurIPS, 2023</em></p> 
                <p>COPRA: an in-context learning agent for theorem-proving in Lean and Coq, leveraging GPT-4 for tactic proposals within a stateful search, outperforming both few-shot GPT-4 and finetuned models on benchmarks.</p>
            </div>
        </div>

        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2106.04015.pdf">Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning</a></h3>
                <p>Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael W. Dusenberry, Sebastian Farquhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah Liu, Zelda Mariet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim G. J. Rudner, <strong>Yeming Wen</strong>, Florian Wenzel, Kevin Murphy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin Gal & Dustin Tran<br>
                <p>High-quality implementations of standard and SOTA methods on a variety of tasks <a href=". https://github.com/google/uncertainty-baselines">[Code]. </a></p>
            </div>
        </div>

        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/abs/1907.02057">Benchmarking Model-Based Reinforcement Learning</a></h3>
                <p>Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, <strong>Yeming Wen</strong>, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel & Jimmy Ba <br>
                <p>Benchmarking several commonly used model-based algorithms <a href=". http://www.cs.toronto.edu/~tingwuwang/mbrl.html">[Code]. </a></p>
            </div>
        </div>

    </section>

    <section class="publications">
        <h2>Peer-Reviewed Papers</h2>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2312.05677.pdf">Batched Low-Rank Adaptation of Foundation Models</a></h3>
                <p><strong>Yeming Wen</strong> & Swarat Chaudhuri <br>
                <em>International Conference on Learning Representations (ICLR), 2024, (<span class="spotlight">Oral, 1.2%</span>)</em></p>
                <p>FLoRA: batched version of Low-Rank Adaptation for foundation models, enabling efficient, personalized adaptations for diverse tasks, demonstrating its effectiveness across multilingual code generation.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2212.09248.pdf">Natural Language to Code Generation in Interactive Data Science Notebooks</a></h3>
                <p>Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, <strong>Yeming Wen</strong>, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov & Charles Sutton <br>
                <em>Association for Computational Linguistics (ACL, 2023)</em></p>
                <p>ARCADE, a benchmark featuring data analysis problems in Jupyter notebooks <a href=". https://github.com/google-research/arcade-nl2code/tree/main">[Code]. </a></p>
            </div>
        </div>

        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://www.jmlr.org/papers/volume24/22-0479/22-0479.pdf">A Simple Approach to Improve Single-model Deep Uncertainty via Distance-awareness</a></h3>
                <p>Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, <strong>Yeming Wen</strong>, Ghassen Jerfel, Zachary Nado, Jasper Snoek, Dustin Tran & Balaji Lakshminarayanan <br>
                <em>Journal of Machine Learning Research (JMLR, 2023)</em></p>
                <p>Spectral-normalized Neural Gaussian Process (SNGP) enhances uncertainty estimation by improving a single network's distance-awareness ability, without the high costs of ensembles and Bayesian neural networks.</p>
            </div>
        </div>     
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://openreview.net/forum?id=yaksQCYcRs">Neural Program Generation Modulo Static Analysis</a></h3>
                <p>Rohan Mukherjee, <strong>Yeming Wen</strong>, Dipak Chaudhari, Thomas Reps, Swarat Chaudhuri & Chris Jermaine <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS), 2021, (<span class="spotlight">Spotlight</span>)</em></p>
                <p>By conditioning on the semantic attributes that are computed on AST nodes by the compiler, our model generates better code snippets such as Java method bodies given their surrounding context.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2010.09875.pdf">Combining Ensembles and Data Augmentation can Harm your Calibration</a></h3>
                <p><strong>Yeming Wen</strong>, Ghassen Jerfel, Rafael Muller, Michael W. Dusenberry, Jasper Snoek, Balaji Lakshminarayanan & Dustin Tran <br>
                <em>International Conference on Learning Representations (ICLR), 2021</em></p>
                <p>By adjusting data augmentation according to calibration, we can exploit both marginalization and invariances.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2005.07186.pdf">Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors</a></h3>
                <p>Michael W. Dusenberry, Ghassen Jerfel, <strong>Yeming Wen</strong>, Yi-An Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan & Dustin Tran <br>
                <em>International Conference on Machine Learning (ICML), 2020</em></p>
                <p>Improved BatchEnsemble with mixture posteriors, Cauchy priors and rank-1 parameterization.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://openreview.net/forum?id=Sklf1yrYDr">BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning</a></h3>
                <p><strong>Yeming Wen</strong>, Dustin Tran & Jimmy Ba <br>
                <em>International Conference on Learning Representations (ICLR), 2020</em><br>
                <em>Bayesian Deep Learning Workshop at NeurIPS, 2019</em></p>
                <p>How to efficiently ensemble deep neural networks efficiently in both computation and memory.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/1902.08234.pdf">An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise</a></h3>
                <p><strong>Yeming Wen</strong>, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan & Jimmy Ba <br>
                <em>International Conference on Artificial Intelligence and Statistics (AISTATS), 2020</em></p>
                <p>How to add noise to gradients with correct covariance structure such that large-batch training generalizes better without longer training.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/abs/1803.04386">Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches</a></h3>
                <p><strong>Yeming Wen</strong>, Paul Vicol, Jimmy Ba, Dustin Tran & Roger Grosse <br>
                <em>International Conference on Learning Representations (ICLR), 2018</em></p>
                <p>How to efficiently make pseudo-independent weight perturbations on mini-batches in evolution strategies and variational BNNs as activation perturbations in dropout.</p>
            </div>
        </div>

    </section>


    <footer>
        Last Update: Feb, 16th, 2024;<br>
        Template: <a href='http://people.eecs.berkeley.edu/~pathak/'>this/that</a>,
                    <a href="http://www.cs.berkeley.edu/~barron/">ce/cette</a>,
                    <a href="http://www.cs.berkeley.edu/~akar/">das/der</a>,
                    <a href="http://www.cs.berkeley.edu/~sgupta/">kono/sono</a> and
                    <a href="http://jeffdonahue.com/">zhege/nage</a>.
    </footer>
</div>
</body>
</html
