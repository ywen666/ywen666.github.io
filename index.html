<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yeming Wen</title>
<!--     <link href='https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Roboto+Mono:wght@400;700&display=swap' rel='stylesheet' type='text/css'> -->
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Montserrat:400,700&display=swap' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <style>
        :root {
            --text-primary: #333;
            --text-secondary: #555;
            --bg-color: #fff;
            --accent-color: #0056b3;
            --hover-accent-color: #003d82;
            --font-primary: 'Lato', sans-serif;
            --font-secondary: 'Roboto Mono', monospace;
        }

        body {
            font-family: var(--font-primary);
            color: var(--text-primary);
            background-color: var(--bg-color);
            margin: 0;
            padding: 0;
            line-height: 1.5;
        }

/*        a {
            color: var(--accent-color);
            text-decoration: none;
        }

        a:hover {
            color: var(--hover-accent-color);
        }*/
        a, a:visited {
          color: #1772d0;
          text-decoration: none;
        }

        a:hover, a:focus {
          color: #f09228;
        }
        .container {
            max-width: 820px;
            margin: auto;
            padding: 40px;
        }

        .profile {
            display: grid;
            grid-template-columns: 150px auto;
            gap: 80px;
            align-items: center;
            margin-bottom: 20px;
        }

        .profile-photo img {
            width: 150px;
            height: 150px;
            border-radius: 75px;
            object-fit: cover;
            /*border: 4px solid var(--accent-color);*/
        }

        .profile-info h1 {
            margin: 0;
            font-size: 2rem;
            color: var(--text-primary);
        }

        .profile-links a {
            display: inline-block;
            margin-right: 10px;
            font-size: 0.9rem;
        }

        .bio {
            font-size: 1rem;
            color: var(--text-secondary);
        }

        .publications {
          margin-top: 0;
        }

        .publication h3 {
          margin-top: 0;
        }

        .spotlight {
          color: red;
        }

        .experience {
          margin-top: 0;
        }

        .experience-entry {
            margin-top: 0;
        }

        footer {
          font-size: 14px;
          color: #777;
        }

        @media (max-width: 768px) {
            .profile {
                grid-template-columns: 1fr;
                text-align: center;
            }

            .profile-photo img {
                margin: 0 auto 20px;
            }

            .profile-info, .profile-links {
                text-align: center;
            }
        }
    </style>
</head>
<body>
<div class="container">
    <div class="profile">
        <div class="profile-photo">
            <img src="ywen.jpg" alt="Yeming Wen">
        </div>
        <div>
            <div class="profile-info">
                <h1>Yeming Wen</h1>
                <p>PhD student at University of Texas at Austin<br>
                ywen@utexas.edu</p>
            </div>
            <div class="profile-links">
            <p><a href="mailto:ywen@utexas.edu">Email</a> /&nbsp&nbsp&nbsp
               <a href="ywen_cv.pdf">CV</a> /&nbsp&nbsp&nbsp
               <a href="https://scholar.google.com/citations?hl=en&user=J2GzNAkAAAAJ">Scholar</a> /&nbsp&nbsp&nbsp
               <a href="https://www.linkedin.com/in/yeming-wen-0480a6a2/">LinkedIn</a></p>
            </div>
        </div>
    </div>
    <div class="bio">
        <p>I am a 4th year computer science PhD student at UT Austin, advised by <a href="https://www.cs.utexas.edu/~swarat/">Prof. Swarat Chaudhuri</a>. My research focuses on building machine learning frameworks to generate code with human-like efficiency. I'm also interested in enhancing the efficient adaptation framework for Large Language Models (LLMs), specifically for code generation tasks. <br>Before joining UT Austin, I was a master student in computer science advised by <a href="https://jimmylba.github.io/"> Prof. Jimmy Ba</a> at University of Toronto. I worked on the development of efficient learning algorithms for deep neural networks. My undergraduate studies were also at the University of Toronto, working with <a href="https://www.cs.toronto.edu/~rgrosse/"> Prof. Roger Grosse</a> on stochastic deep neural networks.</p>
    </div>



    <section class="publications">
        <h2>Preprints</h2>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2402.08073.pdf">Grounding Data Science Code Generation with Input-Output Specifications</a></h3>
                <p><strong>Yeming Wen</strong>, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri & Alex Polozov <br>
                <em>Instruction Tuning and Instruction Following Workshop at NeurIPS, 2023</em></p> 
                <p>We enhance large language models for code generation from natural language prompts with explicit input-output specifications by leveraging synthetic data and utilizing execution-derived feedback.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2310.04353.pdf">An In-Context Learning Agent for Formal Theorem-Proving</a></h3>
                <p>Amitayush Thakur, George Tsoukalas, <strong>Yeming Wen</strong>, Jimmy Xin & Swarat Chaudhuri<br>
                <em>Math-AI Workshop at NeurIPS, 2023</em></p> 
                <p>COPRA: an in-context learning agent for theorem-proving in Lean and Coq, leveraging GPT-4 for tactic proposals within a stateful search, outperforming both few-shot GPT-4 and finetuned models on benchmarks.</p>
            </div>
        </div>

        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2106.04015.pdf">Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning</a></h3>
                <p>Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael W. Dusenberry, Sebastian Farquhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen Jerfel, Jeremiah Liu, Zelda Mariet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim G. J. Rudner, <strong>Yeming Wen</strong>, Florian Wenzel, Kevin Murphy, D. Sculley, Balaji Lakshminarayanan, Jasper Snoek, Yarin Gal & Dustin Tran<br>
                <p>High-quality implementations of standard and SOTA methods on a variety of tasks <a href=". https://github.com/google/uncertainty-baselines">[Code]. </a></p>
            </div>
        </div>

        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/abs/1907.02057">Benchmarking Model-Based Reinforcement Learning</a></h3>
                <p>Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, <strong>Yeming Wen</strong>, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel & Jimmy Ba <br>
                <p>Benchmarking several commonly used model-based algorithms <a href=". http://www.cs.toronto.edu/~tingwuwang/mbrl.html">[Code]. </a></p>
            </div>
        </div>

    </section>

    <section class="publications">
        <h2>Peer-Reviewed Papers</h2>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2312.05677.pdf">Batched Low-Rank Adaptation of Foundation Models</a></h3>
                <p><strong>Yeming Wen</strong> & Swarat Chaudhuri <br>
                <em>International Conference on Learning Representations (ICLR), 2024 (<span class="spotlight">Oral, 1.2%</span>)</em></p>
                <p>FLoRA: batched version of Low-Rank Adaptation (LoRA) for foundation models, enabling efficient, personalized adaptations for diverse tasks, demonstrating its effectiveness across multilingual code generation.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2212.09248.pdf">Natural Language to Code Generation in Interactive Data Science Notebooks</a></h3>
                <p>Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, <strong>Yeming Wen</strong>, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov & Charles Sutton <br>
                <em>Association for Computational Linguistics (ACL, 2023)</em></p>
                <p>Arcade: a benchmark featuring challenging data analysis problems in Jupyter notebooks <a href=". https://github.com/google-research/arcade-nl2code/tree/main">[Code]. </a></p>
            </div>
        </div>

        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://www.jmlr.org/papers/volume24/22-0479/22-0479.pdf">A Simple Approach to Improve Single-model Deep Uncertainty via Distance-awareness</a></h3>
                <p>Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, <strong>Yeming Wen</strong>, Ghassen Jerfel, Zachary Nado, Jasper Snoek, Dustin Tran & Balaji Lakshminarayanan <br>
                <em>Journal of Machine Learning Research (JMLR, 2023)</em></p>
                <p>Spectral-normalized Neural Gaussian Process (SNGP) enhances uncertainty estimation by improving a single network's distance-awareness ability, without the high costs of ensembles and Bayesian neural networks.</p>
            </div>
        </div>     
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://openreview.net/forum?id=yaksQCYcRs">Neural Program Generation Modulo Static Analysis</a></h3>
                <p>Rohan Mukherjee, <strong>Yeming Wen</strong>, Dipak Chaudhari, Thomas Reps, Swarat Chaudhuri & Chris Jermaine <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS), 2021 (<span class="spotlight">Spotlight</span>)</em></p>
                <p>By conditioning on the semantic attributes that are computed on AST nodes by the compiler, our model generates better code snippets such as Java method bodies given their surrounding context.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2010.09875.pdf">Combining Ensembles and Data Augmentation can Harm your Calibration</a></h3>
                <p><strong>Yeming Wen</strong>, Ghassen Jerfel, Rafael Muller, Michael W. Dusenberry, Jasper Snoek, Balaji Lakshminarayanan & Dustin Tran <br>
                <em>International Conference on Learning Representations (ICLR), 2021</em></p>
                <p>CAMixup: by adjusting data augmentation according to calibration, we can exploit both marginalization and invariances.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/2005.07186.pdf">Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors</a></h3>
                <p>Michael W. Dusenberry, Ghassen Jerfel, <strong>Yeming Wen</strong>, Yi-An Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan & Dustin Tran <br>
                <em>International Conference on Machine Learning (ICML), 2020</em></p>
                <p>Improved BatchEnsemble with mixture posteriors, Cauchy priors and rank-1 parameterization.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://openreview.net/forum?id=Sklf1yrYDr">BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning</a></h3>
                <p><strong>Yeming Wen</strong>, Dustin Tran & Jimmy Ba <br>
                <em>International Conference on Learning Representations (ICLR), 2020</em><br>
                <em>Bayesian Deep Learning Workshop at NeurIPS, 2019</em></p>
                <p>How to efficiently ensemble deep neural networks efficiently in both computation and memory.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/pdf/1902.08234.pdf">An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise</a></h3>
                <p><strong>Yeming Wen</strong>, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan & Jimmy Ba <br>
                <em>International Conference on Artificial Intelligence and Statistics (AISTATS), 2020</em></p>
                <p>How to add noise to gradients with correct covariance structure such that large-batch training generalizes better without longer training.</p>
            </div>
        </div>
        <div class="publication-entry">
            <div class="publication-content">
                <h3><a href="https://arxiv.org/abs/1803.04386">Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches</a></h3>
                <p><strong>Yeming Wen</strong>, Paul Vicol, Jimmy Ba, Dustin Tran & Roger Grosse <br>
                <em>International Conference on Learning Representations (ICLR), 2018</em></p>
                <p>How to efficiently make pseudo-independent weight perturbations on mini-batches in evolution strategies and variational BNNs as activation perturbations in dropout.</p>
            </div>
        </div>
    </section>

    <section class="experience">
        <h2>Experience</h2>
        <div class="experience-entry">
            <p>Student Researcher, Google <br>
            2022.09 - 2023.05, Remote <br>
            Host: Alex Polozov</p>
        </div>
        <div class="experience-entry">
            <p>Ph.D. Resident, Google X, the moonshot factory <br>
            2022.05 - 2022.08, Mountain View <br>
            Host: Alex Polozov</p>
        </div>
        <div class="experience-entry">
            <p>Research Intern, Google <br>
            2020.02 - 2020.09, Mountain View <br>
            Host: Dustin Tran</p>
        </div>
        <div class="experience-entry">
            <p>Student Researcher, Google <br>
            2019.08 - 2019.12, Toronto <br>
            Host: Dustin Tran</p>
        </div>
    </section>

    <footer>
        Last Update: Feb, 16th, 2024;<br>
        Template: <a href="http://www.cs.berkeley.edu/~barron/">ce/cette</a>,
                    <a href="http://www.cs.berkeley.edu/~akar/">das/der</a>,
                    <a href="http://www.cs.berkeley.edu/~sgupta/">kono/sono</a> and
                    <a href="http://jeffdonahue.com/">zhege/nage</a>.
    </footer>
</div>
</body>
</html>
